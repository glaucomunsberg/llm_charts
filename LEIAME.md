# ðŸš€ LLM Chat Deployment com LibreChat e Ollama

Este projeto permite hospedar um modelo de linguagem grande (LLM) em uma mÃ¡quina Ubuntu usando Docker. A aplicaÃ§Ã£o usa [LibreChat](https://www.librechat.ai/) como interface web para interaÃ§Ã£o com o modelo [Llama 3.2:1b](https://ollama.com/library/llama3.2:1b), rodando sobre a plataforma [Ollama](https://ollama.com/). AlÃ©m disso, o sistema usa um banco de dados vetorial ([PGVector](https://github.com/pgvector/pgvector)) utilizado pelo [RAG API](https://github.com/danny-avila/rag_api) e o [MongoDB](https://hub.docker.com/_/mongo) para armazenar conversas e dados relevantes.

![screenshot](/data/docs/screenshot-pdf-png.png)

## ðŸ“Œ VisÃ£o Geral da Arquitetura

![sistem_design](/data/docs/system_design.png)

A aplicaÃ§Ã£o Ã© composta pelos seguintes serviÃ§os:

- **Web Chat**: `LibreChat` - Interface web para interaÃ§Ã£o com o LLM.
- **RAG API**: ServiÃ§o responsÃ¡vel pelo processo de recuperaÃ§Ã£o aumentada de geraÃ§Ã£o (RAG).
- **Banco de Dados**:
  - `MongoDB`: Para armazenamento de metadados das conversas.
  - `PGVector (PostgreSQL)`: Para armazenamento vetorial de embeddings.
- **Modelo de LLM**: `Llama 3.2:1b` - Executado via Ollama.
- **Containers**: Todos os serviÃ§os sÃ£o gerenciados pelo `Docker`.
- **Ambiente**: `Ubuntu` como sistema operacional.

## ðŸ”§ PrÃ©-requisitos

Antes de iniciar a instalaÃ§Ã£o, certifique-se de ter os seguintes requisitos atendidos:

- **Ubuntu** instalado e atualizado (24.04).
- **Docker** e **Docker Compose** (27.5) instalados.
- **GPU compatÃ­vel** (opcional visto que o Ollama roda em CPU) para aceleraÃ§Ã£o de inferÃªncia do modelo LLM.

## ðŸ“¥ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

1. **Clone o repositÃ³rio**
   ```bash
   git clone https://github.com/glaucomunsberg/llm_charts
   cd llm_charts
   ```

2. **Configure as variÃ¡veis de ambiente**
   - Copie os arquivos `.env.sample` para `.env` localizados dentro das pastas `docker/livre-chat/`, `docker/pgvector/`, `docker/livre-chat-rag-api/` e `docker/meilisearch`.
   - Edite o valor de `baseURL` no arquivo `chats/libre-chat/librechat.yaml` para o IP da mÃ¡quina UBuntu caso deseje acesso remoto.

3. **Subindo o modelo LLM**

   - Acesse o container do Ollama:
     ```bash
     curl -fsSL https://ollama.com/install.sh | sh
     ```
   
   - Para enviar arquivos para o servidor Ã© preciso passar o parÃ¢metro para zero:
     ```bash
     sudo nano /etc/sysctl.d/10-ptrace.conf
     ```
     ```text
        kernel.yama.ptrace_scope = 0
     ```
     [leia sobre a importÃ¢ncia](https://docs.linaroforge.com/24.0.5/html/forge/general_troubleshooting_appendix/attaching/system_not_connecting_debuggers_fedora_ubuntu.html)
   
   - Habilite o debug: 
     ```bash
        set debuginfod enabled on
     ```

   - Baixando models usados:
   ```bash
        ollama pull nomic-embed-text llama3.2:1b
   ```

   - Inicie o servidor Ollama acesso remoto VocÃª pode ignorar caso nÃ£o queira acesso remoto, pois o serviÃ§o ollama jÃ¡ estÃ¡ rodando na sua mÃ¡quina: 
     ```bash
     set debuginfod enabled on
     sudo service ollama stop
     OLLAMA_HOST="0.0.0.0" OLLAMA_ORIGINS="*" ollama serve
     ```

4. **Suba os containers com Docker Compose**
   ```bash
   docker-compose up -d
   ```
   Isso iniciarÃ¡ todos os serviÃ§os necessÃ¡rios para o funcionamento do chat baseado em LLM.


5. **Acesse a aplicaÃ§Ã£o**
   - Abra o navegador e acesse: [`http://localhost:3080`](http://localhost:3080)
   - Agora vocÃª pode interagir com o modelo de IA via LibreChat.

## ðŸ“‚ Estrutura do Projeto

```
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ livre-chat/                # ConfiguraÃ§Ã£o do LibreChat
â”‚   â”œâ”€â”€ meilisearch/               # ConfiguraÃ§Ã£o do MeiliSearch
â”‚   â”œâ”€â”€ pgvector/                  # ConfiguraÃ§Ã£o do banco de dados vetorial
â”‚   â”œâ”€â”€ livre-chat-rag-api/         # ConfiguraÃ§Ã£o da API RAG
â”œâ”€â”€ chats/                         # Dados persistidos das conversas
â”‚   â”œâ”€â”€ livre-chat/
â”‚   â”‚   â”œâ”€â”€ images/                 # Imagens usadas na interface
â”‚   â”‚   â”œâ”€â”€ logs/                   # Logs da aplicaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ librechat.yaml          # ConfiguraÃ§Ã£o do LibreChat
â”œâ”€â”€ compose.yml                     # Arquivo de configuraÃ§Ã£o do Docker Compose
```

## ðŸ¤– PersonalizaÃ§Ã£o e ExpansÃ£o

Se desejar modificar o modelo de LLM ou alterar as configuraÃ§Ãµes da aplicaÃ§Ã£o, edite os arquivos `.env`, `compose.yml` e `librechat.yaml` conforme necessÃ¡rio.

---

Agora vocÃª tem uma instÃ¢ncia funcional de um chatbot baseado em LLM rodando localmente no Ubuntu com possibilidade de treinamento e personalizaÃ§Ã£o. 

Para mais detalhes sobre acesse os projetos oficiais usados neste projeto:

- [LibreChat](https://www.librechat.ai/)
- [Ollama](https://ollama.com/)
- [RAG API](https://github.com/danny-avila/rag_api)
- [PGVector](https://github.com/pgvector/pgvector)
- [MongoDB](https://hub.docker.com/_/mongo)
- [MeiliSearch](https://www.meilisearch.com/)
- [Docker](https://www.docker.com/)
- [Ubuntu](https://ubuntu.com/)
